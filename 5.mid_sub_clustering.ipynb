{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2b7264c",
   "metadata": {},
   "source": [
    "## 1. AGNES 중분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 덴드로그램 시각화 (2차원일 때만)\n",
    "def plot_dendrogram(data, method=\"average\", metric=\"euclidean\", title=\"Dendrogram\"):\n",
    "    linked = linkage(data, method=method, metric=metric)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    dendrogram(linked)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"Distance\")\n",
    "    plt.show()\n",
    "\n",
    "# 최적 임계값과 실루엣 점수 찾기\n",
    "def find_optimal_threshold(data, low=1.0, high=2.0, max_iter=1000):\n",
    "    best_threshold = None\n",
    "    best_score = -1\n",
    "    best_labels = None\n",
    "    best_n_clusters = None\n",
    "    thresholds = []\n",
    "    silhouette_scores = []\n",
    "    n_clusters_list = []\n",
    "    iter_count = 0\n",
    "    \n",
    "    print(\"▶ Finding the optimal distance_threshold using binary search...\\n\")\n",
    "\n",
    "    while high - low > 0.001 and iter_count < max_iter:\n",
    "        iter_count += 1\n",
    "        mid = (low + high) / 2\n",
    "        agnes = AgglomerativeClustering(n_clusters=None, metric=\"euclidean\", linkage=\"average\", distance_threshold=mid)\n",
    "        labels = agnes.fit_predict(data)\n",
    "        n_clusters = len(np.unique(labels))\n",
    "\n",
    "        if n_clusters > 1:\n",
    "            silhouette = silhouette_score(data, labels, metric=\"euclidean\")\n",
    "            thresholds.append(mid)\n",
    "            silhouette_scores.append(silhouette)\n",
    "            n_clusters_list.append(n_clusters)\n",
    "            print(f\"Distance Threshold: {mid:.3f}, Clusters: {n_clusters}, Silhouette Score: {silhouette:.4f}\")\n",
    "\n",
    "            if silhouette > best_score:\n",
    "                best_score = silhouette\n",
    "                best_threshold = mid\n",
    "                best_labels = labels\n",
    "                best_n_clusters = n_clusters\n",
    "\n",
    "            if silhouette >= best_score:\n",
    "                low = mid\n",
    "            else:\n",
    "                high = mid\n",
    "    # 그래프 출력\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(thresholds, silhouette_scores, marker='o', linestyle='-', color='blue')\n",
    "    plt.title(\"Silhouette Score by Distance Threshold\")\n",
    "    plt.xlabel(\"Distance Threshold\")\n",
    "    plt.ylabel(\"Silhouette Score\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\n▶ Optimal Distance Threshold Found!\")\n",
    "    print(f\"Best distance_threshold: {best_threshold:.3f}, Best Clusters: {best_n_clusters}, Best Silhouette Score: {best_score:.4f}\")\n",
    "    return best_threshold, best_labels, best_score, best_n_clusters\n",
    "\n",
    "# 클러스터 시각화 (2차원일 때만)\n",
    "def plot_silhouette(data, labels, title):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(data[:, 0], data[:, 1], c=labels, cmap=\"viridis\", alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Dimension 1\")\n",
    "    plt.ylabel(\"Dimension 2\")\n",
    "    plt.colorbar(label=\"Cluster Label\")\n",
    "    plt.show()\n",
    "\n",
    "# AGNES 클러스터링 및 통합 처리\n",
    "def process_agnes_embedding(name, data, metadata_df=None):\n",
    "    print(f\"Processing: {name}\")\n",
    "\n",
    "    # 1. 덴드로그램 (2D일 때만)\n",
    "    if data.shape[1] == 2:\n",
    "        print(\"\\n1. Dendrogram Visualization\")\n",
    "        plot_dendrogram(data, method=\"average\", metric=\"euclidean\", title=f\"Dendrogram ({name})\")\n",
    "    else:\n",
    "        print(\"\\n1. Dendrogram skipped (not 2D)\")\n",
    "\n",
    "    # 2. 최적 threshold 탐색\n",
    "    print(\"\\n2. Finding the optimal distance_threshold...\")\n",
    "    best_threshold, best_labels, best_score, best_n_clusters = find_optimal_threshold(data)\n",
    "\n",
    "    # 3. 최종 덴드로그램 (2D일 때만)\n",
    "    if data.shape[1] == 2:\n",
    "        print(\"\\n3. Dendrogram with Optimal Threshold\")\n",
    "        linked = linkage(data, method=\"average\", metric=\"euclidean\")\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        dendrogram(linked, color_threshold=best_threshold)\n",
    "        plt.axhline(y=best_threshold, color=\"r\", linestyle=\"--\", label=f\"Best threshold = {best_threshold:.3f}\")\n",
    "        plt.title(f\"Dendrogram with Optimal Distance Threshold ({name})\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # 4. 클러스터링 결과 시각화 (2D만)\n",
    "    if data.shape[1] == 2:\n",
    "        print(\"\\n4. Final Clustering Result\")\n",
    "        plot_silhouette(data, best_labels, title=f\"Final Clustering ({name})\")\n",
    "    else:\n",
    "        print(\"\\n4. Final Clustering Result skipped (not 2D)\")\n",
    "\n",
    "    # 5. 결과 저장\n",
    "    df_result = pd.DataFrame(data)\n",
    "    df_result['Cluster'] = best_labels\n",
    "\n",
    "    if best_n_clusters > 1:\n",
    "        silhouette_vals = silhouette_samples(data, best_labels, metric=\"euclidean\")\n",
    "        df_result['Silhouette'] = silhouette_vals\n",
    "    else:\n",
    "        df_result['Silhouette'] = -1\n",
    "\n",
    "    # 메타데이터 병합\n",
    "    if metadata_df is not None:\n",
    "        df_result = pd.concat([metadata_df.reset_index(drop=True), df_result], axis=1)\n",
    "\n",
    "    # 리턴\n",
    "    return {\n",
    "        'name': name,\n",
    "        'best_threshold': best_threshold,\n",
    "        'n_clusters': best_n_clusters,\n",
    "        'silhouette_score': best_score,\n",
    "        'data': df_result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003dea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agnes_subcluster_for_id(cluster_id, df, embedding_cols):\n",
    "    cluster_data = df[df['Cluster'] == cluster_id]\n",
    "    X_cluster = cluster_data[embedding_cols].values\n",
    "\n",
    "    try:\n",
    "        _, best_labels, _, _ = find_optimal_threshold(X_cluster)\n",
    "        return pd.Series(best_labels, index=cluster_data.index)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Cluster {cluster_id}: {e}\")\n",
    "        return pd.Series([-1] * len(cluster_data), index=cluster_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f81844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 부분 위 코드 cosine으로 바꿔서 다시 돌려봐야함 (시간이 오래 걸림)\n",
    "\n",
    "file_path = \"./ref_origin_hi_clu.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 필요한 컬럼만 추출\n",
    "embedding_cols = [str(i) for i in range(64)]\n",
    "X = df[embedding_cols].values\n",
    "\n",
    "# 클러스터링 수행에 필요한 패키지 로드\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# 기존 BIRCH 클러스터 ID 확인\n",
    "cluster_ids = df['Cluster'].unique()\n",
    "\n",
    "# AGNES 결과 저장\n",
    "agnes_all_results = []\n",
    "\n",
    "# 각 BIRCH 클러스터별로 AGNES 수행\n",
    "print(\"▶ Starting AGNES sub-clustering (parallelized)...\\n\")\n",
    "agnes_all_results = Parallel(n_jobs=-1)(\n",
    "    delayed(agnes_subcluster_for_id)(cid, df, embedding_cols)\n",
    "    for cid in tqdm(df['Cluster'].unique())\n",
    ")\n",
    "\n",
    "# 결과 통합\n",
    "df['AGNES_Subcluster'] = pd.concat(agnes_all_results).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6cfe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf6929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subcluster 칼럼 이름 변경\n",
    "df.rename(columns={'AGNES_Subcluster': 'Subcluster'}, inplace=True)\n",
    "df.to_csv('./ref_origin_emb_red_clu_mid.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfbe533",
   "metadata": {},
   "source": [
    "## 2. AGNES 중분류 결과 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa04357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "df = pd.read_csv(\"./ref_origin_emb_red_clu_mid.csv\")  # AGNES 클러스터링 포함\n",
    "ref_df = pd.read_csv(\"./ref_origin_emb_red.csv\")  # 원본 keyword 포함\n",
    "\n",
    "# 2. 컬럼 이름 변경\n",
    "df.rename(columns={\n",
    "    'Cluster': 'Topcluster',\n",
    "    'Subcluster': 'Midcluster'\n",
    "}, inplace=True)\n",
    "\n",
    "# 3. Midcluster ID 생성\n",
    "df['Midcluster_ID'] = df['Topcluster'].astype(str) + \"_\" + df['Midcluster'].astype(str)\n",
    "\n",
    "# 4. 중심좌표 계산\n",
    "embedding_cols = [str(i) for i in range(64)]\n",
    "midcluster_centers = df.groupby(\"Midcluster_ID\")[embedding_cols].mean()\n",
    "\n",
    "# 5. 유사도 행렬 계산\n",
    "similarity_matrix = cosine_similarity(midcluster_centers)\n",
    "similarity_df = pd.DataFrame(similarity_matrix,\n",
    "                             index=midcluster_centers.index,\n",
    "                             columns=midcluster_centers.index).reset_index()\n",
    "\n",
    "# 6. 유사도 melt\n",
    "similarity_melted = similarity_df.melt(id_vars='Midcluster_ID',\n",
    "                                       var_name='Compared_Midcluster',\n",
    "                                       value_name='Cosine_Similarity')\n",
    "\n",
    "# 7. top 3 유사도 추출\n",
    "top_similar_expanded = (\n",
    "    similarity_melted[similarity_melted['Midcluster_ID'] != similarity_melted['Compared_Midcluster']]\n",
    "    .sort_values(['Midcluster_ID', 'Cosine_Similarity'], ascending=[True, False])\n",
    "    .groupby('Midcluster_ID')\n",
    "    .head(3)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "top_similar_expanded['Rank'] = top_similar_expanded.groupby('Midcluster_ID').cumcount() + 1\n",
    "top_similar_pivot = top_similar_expanded.pivot(index='Midcluster_ID', columns='Rank',\n",
    "                                                values=['Compared_Midcluster', 'Cosine_Similarity'])\n",
    "top_similar_pivot.columns = [f\"Most_Similar_{col[1]}\" if col[0] == 'Compared_Midcluster'\n",
    "                             else f\"Similarity_{col[1]}\" for col in top_similar_pivot.columns]\n",
    "top_similar_pivot.reset_index(inplace=True)\n",
    "\n",
    "# 8. 키워드 컬럼 추출\n",
    "keyword_col = next((col for col in ref_df.columns if 'keyword' in col.lower()), None)\n",
    "\n",
    "# 9. 클러스터-미드클러스터 매핑\n",
    "merged = pd.merge(\n",
    "    df[['ticket_id_hashed', 'Topcluster', 'Midcluster']],\n",
    "    ref_df[['ticket_id_hashed', keyword_col]],\n",
    "    on='ticket_id_hashed',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 10. 키워드 집계\n",
    "grouped_keywords = (\n",
    "    merged.groupby([\"Topcluster\", \"Midcluster\"])[keyword_col]\n",
    "    .apply(lambda x: pd.Series(x.dropna().explode().str.split(\",\")).explode().str.strip().value_counts())\n",
    "    .reset_index()\n",
    ")\n",
    "grouped_keywords.columns = [\"Topcluster\", \"Midcluster\", \"Keyword\", \"Count\"]\n",
    "grouped_keywords['Midcluster_ID'] = grouped_keywords['Topcluster'].astype(str) + \"_\" + grouped_keywords['Midcluster'].astype(str)\n",
    "\n",
    "# 11. 최종 병합 및 저장\n",
    "final_keywords_top3 = pd.merge(grouped_keywords, top_similar_pivot, on='Midcluster_ID', how='left')\n",
    "final_keywords_top3.to_csv(\"./midcluster_top_keywords_with_top3_similarity.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e137e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "midcluster = pd.read_csv('./midcluster_top_keywords_with_top3_similarity.csv')\n",
    "midcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08499873",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = midcluster['Midcluster_ID'].unique()\n",
    "print('Unique Midcluster_IDs:', unique_ids)\n",
    "print('Number of unique Midcluster_IDs:', len(unique_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1743345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대분류 클러스터 크기 계산\n",
    "cluster_sizes = df['Topcluster'].value_counts().sort_index()\n",
    "print(\"대분류 클러스터 크기:\", cluster_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b956855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서브클러스터 크기 계산\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "subcluster_sizes = df['Midcluster_ID'].value_counts().sort_index()\n",
    "print(\"서브클러스터 크기:\", subcluster_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f8c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서브클러스터 크기 시각화\n",
    "subcluster_sizes.plot(kind='bar', figsize=(12, 6), title='Subcluster Sizes')\n",
    "plt.xlabel('Midcluster ID')\n",
    "plt.ylabel('Size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecaf1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts = df['Midcluster_ID'].value_counts()\n",
    "\n",
    "min_samples = 100  # 입력\n",
    "target_clusters = cluster_counts[cluster_counts >= min_samples].index.tolist()\n",
    "print(f\"소분류 대상 클러스터 개수 : {len(target_clusters)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145fbe52",
   "metadata": {},
   "source": [
    "## 3. AGNES 소분류 (AgglomerativeClustering + distance_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bf6290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 결과 저장용 컬럼 초기화\n",
    "df['Subcluster'] = 0\n",
    "sub_keywords_and_texts = {}\n",
    "\n",
    "def auto_cut_dendrogram(data, method=\"average\", metric=\"cosine\", plot=False, cluster_name=\"\"):\n",
    "    linked = linkage(data, method=method, metric=metric)\n",
    "    distances = linked[:, 2]\n",
    "    deltas = np.diff(distances)\n",
    "    threshold = distances[np.argmax(deltas)]\n",
    "\n",
    "    labels = fcluster(linked, t=threshold, criterion=\"distance\")\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        dendrogram(\n",
    "            linked,\n",
    "            no_labels=True,\n",
    "            color_threshold=0,              # 색상 구분 제거\n",
    "            above_threshold_color='black'   # 모든 선 검정색\n",
    "        )\n",
    "        plt.axhline(y=threshold, color='red', linestyle='--', label=f\"cut@{threshold:.2f}\")\n",
    "        plt.title(f\"Dendrogram: {cluster_name} (Clusters={len(set(labels))})\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return labels, threshold\n",
    "\n",
    "# 벡터 컬럼 (64차원 임베딩)\n",
    "vec_cols = list(map(str, range(64)))\n",
    "\n",
    "# Topcluster → Midcluster 단위로 AGNES 소분류 수행\n",
    "for top_id in sorted(df['Topcluster'].unique()):\n",
    "    top_df = df[df['Topcluster'] == top_id]\n",
    "\n",
    "    for mid_id in sorted(top_df['Midcluster'].unique()):\n",
    "        sub_df = top_df[top_df['Midcluster'] == mid_id]\n",
    "        if len(sub_df) <= 100:\n",
    "            continue  # 소분류 의미 없을 정도로 작으면 스킵\n",
    "\n",
    "        vectors = sub_df[vec_cols].values\n",
    "        texts = sub_df['generated_summary'].tolist()\n",
    "        indices = sub_df.index.tolist()\n",
    "\n",
    "        # AGNES 기반 덴드로그램 컷팅 + 시각화\n",
    "        sub_labels, threshold = auto_cut_dendrogram(\n",
    "            data=vectors,\n",
    "            plot=True,\n",
    "            cluster_name=f\"Top {top_id} - Mid {mid_id}\"\n",
    "        )\n",
    "\n",
    "        # 소분류 레이블 저장\n",
    "        df.loc[indices, 'Subcluster'] = sub_labels\n",
    "\n",
    "        # 소분류별 키워드 추출\n",
    "        for sub_id in sorted(set(sub_labels)):\n",
    "            sub_idx = [i for i, l in enumerate(sub_labels) if l == sub_id]\n",
    "            sub_texts = [texts[i] for i in sub_idx]\n",
    "            if len(sub_texts) < 3:\n",
    "                continue  # LDA 돌리기에는 너무 적음\n",
    "\n",
    "            tfidf = TfidfVectorizer(max_features=30)\n",
    "            X_tfidf = tfidf.fit_transform(sub_texts)\n",
    "            tfidf_keywords = tfidf.get_feature_names_out()\n",
    "\n",
    "            lda = LatentDirichletAllocation(n_components=1, random_state=42)\n",
    "            lda.fit(X_tfidf)\n",
    "            lda_keywords = [\n",
    "                tfidf.get_feature_names_out()[i]\n",
    "                for i in lda.components_[0].argsort()[-10:][::-1]\n",
    "            ]\n",
    "\n",
    "            sub_keywords_and_texts[f\"{top_id}-{mid_id}-{sub_id}\"] = {\n",
    "                'TFIDF': tfidf_keywords.tolist(),\n",
    "                'LDA': lda_keywords\n",
    "            }\n",
    "\n",
    "# Subcluster ID 생성\n",
    "df['Subcluster_ID'] = (\n",
    "    df['Topcluster'].astype(str) + \"_\" +\n",
    "    df['Midcluster'].astype(str) + \"_\" +\n",
    "    df['Subcluster'].astype(str)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ede8539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 한글 폰트 설정 (macOS 기준)\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 결과 저장용 초기화\n",
    "df['Subcluster'] = 0\n",
    "sub_keywords_and_texts = {}\n",
    "\n",
    "# 병합 거리 변화량 시각화 함수\n",
    "def plot_deltas(deltas, threshold_index, cluster_name):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(range(1, len(deltas) + 1), deltas, label='거리 증가량 (delta)')\n",
    "    plt.axvline(x=threshold_index + 1, color='red', linestyle='--', label='최대 증가 지점')\n",
    "    plt.xlabel(\"병합 단계\")\n",
    "    plt.ylabel(\"거리 증가량\")\n",
    "    plt.title(f\"병합 거리 변화량: {cluster_name}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 병합 거리 증가량 및 임계값 계산 함수\n",
    "def show_linkage_distances(linked):\n",
    "    distances = linked[:, 2]\n",
    "    deltas = np.diff(distances)\n",
    "    threshold_index = np.argmax(deltas)\n",
    "    threshold_value = distances[threshold_index]\n",
    "\n",
    "    max_increase_flags = [False] * len(distances)\n",
    "    if threshold_index + 1 < len(distances):\n",
    "        max_increase_flags[threshold_index + 1] = True  # delta 최대 줄 (파란색 표시)\n",
    "\n",
    "    df_merge = pd.DataFrame({\n",
    "        \"병합번호\": range(1, len(distances) + 1),\n",
    "        \"병합거리 (distance)\": distances,\n",
    "        \"거리 증가량 (delta)\": [np.nan] + deltas.tolist(),\n",
    "        \"컷팅 기준 여부 (distance[i])\": [i == threshold_index for i in range(len(distances))],\n",
    "        \"delta 최대 여부 (distance[i+1])\": max_increase_flags\n",
    "    })\n",
    "\n",
    "    return df_merge, threshold_index, threshold_value\n",
    "\n",
    "# 병합 테이블 Top10 + 색상 강조 출력 함수\n",
    "def display_top_deltas_with_max(df_merge, threshold_index):\n",
    "    delta_max_row_index = threshold_index + 1\n",
    "    distance_cut_row_index = threshold_index\n",
    "\n",
    "    top_deltas = df_merge.sort_values(\"거리 증가량 (delta)\", ascending=False).head(9)\n",
    "    delta_max_row = df_merge.iloc[[delta_max_row_index]]\n",
    "    distance_cut_row = df_merge.iloc[[distance_cut_row_index]]\n",
    "    merged = pd.concat([top_deltas, delta_max_row, distance_cut_row]).drop_duplicates(subset=[\"병합번호\"])\n",
    "\n",
    "    def highlight_row(row):\n",
    "        if row.name == delta_max_row_index:\n",
    "            return ['background-color: blue'] * len(row)\n",
    "        elif row.name == distance_cut_row_index:\n",
    "            return ['background-color: red'] * len(row)\n",
    "        else:\n",
    "            return [''] * len(row)\n",
    "\n",
    "    styled = merged.sort_values(\"거리 증가량 (delta)\", ascending=False).style.apply(highlight_row, axis=1)\n",
    "    display(styled)\n",
    "\n",
    "# 벡터 컬럼 정의\n",
    "vec_cols = list(map(str, range(64)))\n",
    "\n",
    "# Topcluster → Midcluster 단위 AGNES 수행\n",
    "for top_id in sorted(df['Topcluster'].unique()):\n",
    "    top_df = df[df['Topcluster'] == top_id]\n",
    "\n",
    "    for mid_id in sorted(top_df['Midcluster'].unique()):\n",
    "        sub_df = top_df[top_df['Midcluster'] == mid_id]\n",
    "        if len(sub_df) <= 100:\n",
    "            continue  # 너무 작으면 스킵\n",
    "\n",
    "        vectors = sub_df[vec_cols].values\n",
    "        texts = sub_df['generated_summary'].tolist()\n",
    "        indices = sub_df.index.tolist()\n",
    "\n",
    "        # linkage 및 임계값 계산\n",
    "        linked = linkage(vectors, method=\"average\", metric=\"cosine\")\n",
    "        df_merge, threshold_index, threshold_value = show_linkage_distances(linked)\n",
    "        deltas = np.diff(linked[:, 2])\n",
    "\n",
    "        cluster_name = f\"Top {top_id} - Mid {mid_id}\"\n",
    "        plot_deltas(deltas, threshold_index, cluster_name=cluster_name)\n",
    "        display_top_deltas_with_max(df_merge, threshold_index)\n",
    "\n",
    "        # 덴드로그램 컷팅\n",
    "        labels = fcluster(linked, t=threshold_value, criterion=\"distance\")\n",
    "        df.loc[indices, 'Subcluster'] = labels\n",
    "\n",
    "        # 키워드 추출\n",
    "        for sub_id in sorted(set(labels)):\n",
    "            sub_idx = [i for i, l in enumerate(labels) if l == sub_id]\n",
    "            sub_texts = [texts[i] for i in sub_idx]\n",
    "            if len(sub_texts) < 3:\n",
    "                continue\n",
    "\n",
    "            tfidf = TfidfVectorizer(max_features=30)\n",
    "            X_tfidf = tfidf.fit_transform(sub_texts)\n",
    "            tfidf_keywords = tfidf.get_feature_names_out()\n",
    "\n",
    "            lda = LatentDirichletAllocation(n_components=1, random_state=42)\n",
    "            lda.fit(X_tfidf)\n",
    "            lda_keywords = [\n",
    "                tfidf.get_feature_names_out()[i]\n",
    "                for i in lda.components_[0].argsort()[-10:][::-1]\n",
    "            ]\n",
    "\n",
    "            sub_keywords_and_texts[f\"{top_id}-{mid_id}-{sub_id}\"] = {\n",
    "                'TFIDF': tfidf_keywords.tolist(),\n",
    "                'LDA': lda_keywords\n",
    "            }\n",
    "\n",
    "# Subcluster ID 최종 생성\n",
    "df['Subcluster_ID'] = (\n",
    "    df['Topcluster'].astype(str) + \"_\" +\n",
    "    df['Midcluster'].astype(str) + \"_\" +\n",
    "    df['Subcluster'].astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a96a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "# Topcluster, Midcluster, Subcluster 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aab1a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대분류 개수\n",
    "df['Topcluster'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a2c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중분류 개수\n",
    "df['Midcluster_ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc29898",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Subcluster_ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e364a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subcluster_ID별 개수 세기\n",
    "subcluster_counts = df['Subcluster_ID'].value_counts().sort_index()\n",
    "print(subcluster_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de8f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./ref_origin_emb_red_clu_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109ef735",
   "metadata": {},
   "source": [
    "## 4. TFIDF & LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613e54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from gensim import corpora, models\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk 리소스 다운로드\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "### 1. TF-IDF 키워드 추출 함수\n",
    "def extract_keywords(texts, top_n=5):\n",
    "    if len(texts) == 0:\n",
    "        return [\"No Keywords\"]\n",
    "    try:\n",
    "        stop_words = text.ENGLISH_STOP_WORDS\n",
    "        excluded_words = ['refrigerator', 'ref', 'user', 'suggests', 'suggest', 'suggestion']\n",
    "        stop_words = list(stop_words.union(set(excluded_words)))\n",
    "        vectorizer = TfidfVectorizer(max_df=0.85, min_df=1, stop_words=stop_words)\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        avg_tfidf = tfidf_matrix.mean(axis=0).A1\n",
    "        keywords = [feature_names[i] for i in avg_tfidf.argsort()[-top_n:]]\n",
    "        return keywords\n",
    "    except ValueError as e:\n",
    "        print(f\"Error while extracting keywords: {e}\")\n",
    "        return [\"Error\"]\n",
    "\n",
    "### 2. LDA 토픽 모델링 함수\n",
    "def preprocess_texts(texts, extra_stopwords=None):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    if extra_stopwords:\n",
    "        stop_words = stop_words.union(set(extra_stopwords))\n",
    "\n",
    "    processed = []\n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(text.lower(), preserve_line=True)\n",
    "        tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "        processed.append(tokens)\n",
    "    return processed\n",
    "\n",
    "def cluster_topic_modeling(final_df, cluster_col, text_col, num_topics=1, num_words=5, excluded_words=None):\n",
    "    clusters = final_df[cluster_col].unique()\n",
    "    cluster_topics = {}\n",
    "\n",
    "    for cluster in clusters:\n",
    "        cluster_texts = final_df[final_df[cluster_col] == cluster][text_col].dropna().tolist()\n",
    "        cluster_texts = [text for text in cluster_texts if text.strip()]\n",
    "        processed_texts = preprocess_texts(cluster_texts, extra_stopwords=excluded_words)\n",
    "\n",
    "        if not processed_texts or all(len(text) == 0 for text in processed_texts):\n",
    "            cluster_topics[cluster] = [\"No topic\"]\n",
    "            continue\n",
    "\n",
    "        dictionary = corpora.Dictionary(processed_texts)\n",
    "        corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "\n",
    "        if len(dictionary.token2id) == 0:\n",
    "            cluster_topics[cluster] = [\"No topic\"]\n",
    "            continue\n",
    "\n",
    "        lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "        topics = lda_model.print_topics(num_words=num_words)\n",
    "        topic_keywords = [t[1] for t in topics]\n",
    "        cluster_topics[cluster] = topic_keywords\n",
    "\n",
    "    return cluster_topics\n",
    "\n",
    "### 3. TF-IDF + LDA 병합\n",
    "def merge_keywords_and_topics(tfidf_dict, topic_dict):\n",
    "    all_clusters = sorted(set(tfidf_dict.keys()).union(set(topic_dict.keys())))\n",
    "    merged_data = []\n",
    "\n",
    "    for cluster in all_clusters:\n",
    "        tfidf_keywords = ', '.join(tfidf_dict.get(cluster, [\"No Keywords\"]))\n",
    "        topic_keywords = ', '.join(topic_dict.get(cluster, [\"No topic\"]))\n",
    "        merged_data.append((cluster, tfidf_keywords, topic_keywords))\n",
    "\n",
    "    merged_df = pd.DataFrame(merged_data, columns=[\"Cluster\", \"TFIDF_Keywords\", \"LDA_Topic_Keywords\"])\n",
    "    return merged_df\n",
    "\n",
    "### 4. 클러스터별 키워드 추출 (TF-IDF 전용)\n",
    "def get_cluster_keywords(final_df, cluster_col, text_col, top_n=5):\n",
    "    clusters = final_df[cluster_col].unique()\n",
    "    cluster_keywords = {}\n",
    "    for cluster in clusters:\n",
    "        cluster_texts = final_df[final_df[cluster_col] == cluster][text_col].dropna().tolist()\n",
    "        cluster_texts = [text for text in cluster_texts if text.strip()]\n",
    "        keywords = extract_keywords(cluster_texts, top_n)\n",
    "        cluster_keywords[cluster] = keywords\n",
    "    return cluster_keywords\n",
    "\n",
    "### 5. 실행 파이프라인\n",
    "def run_pipeline(final_df, level_name, cluster_col, text_col):\n",
    "    print(f\"\\n▶ Processing: {level_name}\")\n",
    "    tfidf_keywords = get_cluster_keywords(final_df, cluster_col, text_col)\n",
    "    topic_keywords = cluster_topic_modeling(final_df, cluster_col, text_col, excluded_words=excluded_words)\n",
    "    merged_df = merge_keywords_and_topics(tfidf_keywords, topic_keywords)\n",
    "\n",
    "    output_path = f\"./ref_{level_name}_TFIDF_LDA.csv\"\n",
    "    merged_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"저장 완료: {output_path}\")\n",
    "\n",
    "text_col = 'generated_summary'\n",
    "excluded_words = ['refrigerator', 'ref', 'user', 'suggests', 'suggest', 'suggestion']\n",
    "\n",
    "### 6. 메인 실행\n",
    "if __name__ == \"__main__\":\n",
    "    # 이미 final_df가 메모리에 존재한다고 가정\n",
    "    text_col = 'generated_summary'\n",
    "    excluded_words = ['refrigerator', 'ref', 'user', 'suggests', 'suggest', 'suggestion']\n",
    "    \n",
    "    # Midcluster / Subcluster 단위 분석 실행\n",
    "    run_pipeline(df, \"Topcluster\", \"Topcluster\", text_col)\n",
    "    run_pipeline(df, \"Midcluster\", \"Midcluster_ID\", text_col)\n",
    "    run_pipeline(df, \"Subcluster\", \"Subcluster_ID\", text_col)\n",
    "\n",
    "    # 에러발생 이유: texts의 개수가 매우 적은 경우 (예: Subcluster 안에 문서가 1~2개뿐인 경우)\n",
    "    # 1~2개여도 괜찮다면 좀 더 유하게 조정 or subcluster 자체를 하지말기 등 고민 필요"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
